{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9091d42d-2c4c-4cdc-89df-8a908bbc80c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root_dir: /Users/mh/Downloads/Mini Project/Jan_TextClassification_Iteration\n",
      "results_dir: /Users/mh/Downloads/Mini Project/Jan_TextClassification_Iteration/results\n",
      "models_dir: /Users/mh/Downloads/Mini Project/Jan_TextClassification_Iteration/models\n",
      "logs_dir: /Users/mh/Downloads/Mini Project/Jan_TextClassification_Iteration/logs\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports & Config\n",
    "\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def nowstamp():\n",
    "    return datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Project\n",
    "    project_name: str = \"distilbert_tweet_eval_sentiment\"\n",
    "\n",
    "    # Dataset\n",
    "    dataset_name: str = \"tweet_eval\"\n",
    "    dataset_config: str = \"sentiment\"\n",
    "    text_col: str = \"text\"\n",
    "    label_col: str = \"label\"\n",
    "\n",
    "    # Model\n",
    "    model_name: str = \"distilbert-base-uncased\"\n",
    "    max_seq_len: int = 256 #128\n",
    "\n",
    "    # Train\n",
    "    batch_size: int = 16\n",
    "    lr: float = 1e-5 #2e-5\n",
    "    epochs: int = 3 #2\n",
    "\n",
    "    seed: int = 42\n",
    "\n",
    "    # Speed control (Mac向け)\n",
    "    max_train_examples: int | None = 8000\n",
    "    max_val_examples: int | None = 1000\n",
    "    max_test_examples: int | None = 2000\n",
    "\n",
    "    debug_max_steps_per_epoch: int | None = 300  # None で制限なし\n",
    "\n",
    "    # Early stopping\n",
    "    early_stopping_patience: int = 1\n",
    "\n",
    "    # Output directories (絶対パス)\n",
    "    root_dir: str = \"/Users/mh/Downloads/Mini Project/Jan_TextClassification_Iteration\"\n",
    "    results_dir: str = \"\"\n",
    "    models_dir: str = \"\"\n",
    "    logs_dir: str = \"\"\n",
    "\n",
    "\n",
    "cfg = Config()\n",
    "cfg.results_dir = os.path.join(cfg.root_dir, \"results\")\n",
    "cfg.models_dir   = os.path.join(cfg.root_dir, \"models\")\n",
    "cfg.logs_dir     = os.path.join(cfg.root_dir, \"logs\")\n",
    "\n",
    "\n",
    "os.makedirs(cfg.results_dir, exist_ok=True)\n",
    "os.makedirs(cfg.models_dir, exist_ok=True)\n",
    "os.makedirs(cfg.logs_dir, exist_ok=True)\n",
    "\n",
    "print(\"root_dir:\", cfg.root_dir)\n",
    "print(\"results_dir:\", cfg.results_dir)\n",
    "print(\"models_dir:\", cfg.models_dir)\n",
    "print(\"logs_dir:\", cfg.logs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11503f63-3078-429e-89b1-1ff8412ea8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: mps\n",
      "torch: 2.5.1\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Seed fixed & DEVICE\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(cfg.seed)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "print(\"torch:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "435e77d8-5f7a-4db6-82e5-038ab52c7e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 45615\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 12284\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "})\n",
      "sizes: 8000 1000 2000\n",
      "sample: {'text': '\"QT @user In the original draft of the 7th book, Remus Lupin survived the Battle of Hogwarts. #HappyBirthdayRemusLupin\"', 'label': 2}\n",
      "num_labels: 3\n",
      "label_names: ['negative', 'neutral', 'positive']\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load dataset & take_n\n",
    "\n",
    "def take_n(d, n):\n",
    "    \"\"\"\n",
    "    Datasetを軽量化する関数。\n",
    "    n が None ならそのまま、n が指定されていれば先頭 n 件だけにする。\n",
    "    \"\"\"\n",
    "    if n is None:\n",
    "        return d\n",
    "    n = min(n, len(d))\n",
    "    return d.select(range(n))\n",
    "\n",
    "raw: DatasetDict = load_dataset(cfg.dataset_name, cfg.dataset_config)\n",
    "print(raw)\n",
    "\n",
    "train_ds = raw[\"train\"]\n",
    "val_ds   = raw[\"validation\"]\n",
    "test_ds  = raw[\"test\"]\n",
    "\n",
    "# 軽量化（Macで落ちないように）\n",
    "train_ds = take_n(train_ds, cfg.max_train_examples)\n",
    "val_ds   = take_n(val_ds, cfg.max_val_examples)\n",
    "test_ds  = take_n(test_ds, cfg.max_test_examples)\n",
    "\n",
    "print(\"sizes:\", len(train_ds), len(val_ds), len(test_ds))\n",
    "print(\"sample:\", train_ds[0])\n",
    "\n",
    "# labels info\n",
    "# tweet_eval(sentiment) は label が ClassLabel のはず\n",
    "label_feature = train_ds.features[cfg.label_col]\n",
    "try:\n",
    "    num_labels = int(label_feature.num_classes)\n",
    "    label_names = label_feature.names\n",
    "except Exception:\n",
    "    # 万一 ClassLabel でなければ fallback\n",
    "    labels = sorted(set(train_ds[cfg.label_col]))\n",
    "    num_labels = len(labels)\n",
    "    label_names = [str(x) for x in labels]\n",
    "\n",
    "print(\"num_labels:\", num_labels)\n",
    "print(\"label_names:\", label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c6fb58a-b069-42b0-b5ff-9240c1df35e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0bb648e96f64754b953bfe387110050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "537e16a441ec429ab3a366b116e68c9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a542c8bf6234a41b6b6de6a8748f137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized keys: ['text', 'label', 'input_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Tokenizer & Tokenize\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_name, use_fast=True)\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(\n",
    "        batch[cfg.text_col],\n",
    "        truncation=True,\n",
    "        max_length=cfg.max_seq_len,\n",
    "    )\n",
    "\n",
    "# batched=True を使うと速い & 安定\n",
    "train_tok = train_ds.map(tokenize_batch, batched=True)\n",
    "val_tok   = val_ds.map(tokenize_batch, batched=True)\n",
    "test_tok  = test_ds.map(tokenize_batch, batched=True)\n",
    "\n",
    "# 重要：Trainerを使わず自前DataLoaderするので、torch形式に\n",
    "cols = [\"input_ids\", \"attention_mask\", cfg.label_col]\n",
    "train_tok.set_format(type=\"torch\", columns=cols)\n",
    "val_tok.set_format(type=\"torch\", columns=cols)\n",
    "test_tok.set_format(type=\"torch\", columns=cols)\n",
    "\n",
    "print(\"tokenized keys:\", train_tok.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fbbca7a-c2c2-4322-931b-fee0ed7349b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch keys: KeysView({'input_ids': tensor([[  101,  4931,  1030,  5310,  6616,  2017,  1012,  1045,  2123,  1005,\n",
      "          1056,  2031,  4274,  2127,  9432,  1998,  2026, 12202,  2028,  2180,\n",
      "          1005,  1056,  2130,  2292,  2033,  3696,  3031,  2026,  4070,  2125,\n",
      "          4179,  1012,   102,     0,     0,     0],\n",
      "        [  101,  3892,  1005,  1055,  2792,  1997,  6928,  2305,  6315,  2001,\n",
      "          5507, 21358,  1010,  1996, 12648,  3337,  1010, 26220, 22012,  1010,\n",
      "          1998, 12072,   999,   999,  2066, 10930,  1059,  2705,   999,   102,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2280,  8096, 17207,  5313,  2447,  1999,  1996,  8987, 17549,\n",
      "          2136,  1998,  4916,  2248,  6583,  9905, 17343, 22079,  2098,  2091,\n",
      "          2197,  2305,  2648,  3290,  2103,  1012,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2339,  2003,  3782,  8945,  4095,  2061,  6616,  2378,  1032,\n",
      "         23343, 24096,  2683,  9200,  1011,  1035,  1035,  1011,  1996,  2088,\n",
      "          2089,  2196,  2113,  1012,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1030,  5310,  1030,  5310,  1045,  6346,  2613,  6921,  1010,\n",
      "          2027,  2215,  2032,  2012,  1037,  8276,  7408,  2030,  2489,  1999,\n",
      "          5553,  1012,  1000,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1000,  9119, 12810,  3388,  8840,  2213,  1011,  1996,  3203,\n",
      "         15872,  2545,  1010, 21251,  7874,  1010,  1998,  1037,  2915,  1999,\n",
      "          1996,  2601,  1010,  2008,  2052,  2191,  1037,  2307,  5095,  5027,\n",
      "          6420,  3021,  1000,   102,     0,     0],\n",
      "        [  101,  2040,  2003,  2183,  2000,  1996,  1001,  2142,  6499, 26698,\n",
      "         11253, 10354, 14735,  4164,  4826,  2012, 12415,  8232, 16392,  1029,\n",
      "         10507,  1030,  5310,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2065,  2017,  2228,  4172,  2072,  1005,  1055,  4632,  2003,\n",
      "          2062,  2084,  1015,  1013,  5893,  2613,  2017,  2024, 11865,  2226,\n",
      "         12722,  2098,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [  101, 14764,  1024, 11695, 10995,  2051,  4515,  2006,  5095,  2305,\n",
      "          1013,  4465,  2851,  1012,  2275,  2017, 20940,  2067,  2019,  3178,\n",
      "          1998,  5959,  2019,  1012,  1012,  1012,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1000,  1045,  2428,  2066,  1996,  2126,  6498,  5226,  2003,\n",
      "         11820,  1996,  7151,  1012,  2062,  7151,  3177,  1012,  2130,  2295,\n",
      "          2002,  1047,  1005,  1040,  1999,  1996,  3822,  1010,  2002,  2081,\n",
      "          2070,  6355, 18755,  1000,   102,     0],\n",
      "        [  101,  1030,  5310,  7632,  2129,  2024,  2017,  2085,  2020,  2024,\n",
      "          2017,  2572,  2067,  2000,  2859,  2005,  1041,  3593,  2044,  1041,\n",
      "          3593, 10047,  3740,  2000,  2175,  3531,  2655,  2033,  2089,  2193,\n",
      "          5989, 22610,  2575, 18613, 24087,   102],\n",
      "        [  101, 17244,  2271,  2056,  2009,  2089,  2022,  1016,  1011,  1017,\n",
      "          2062,  2706,  2077,  1045,  2064,  2202,  2026,  2482,  2000,  1996,\n",
      "         11033,  2005,  1996,  9131,  2006, 15887,  1996,  1012,  1012,  1012,\n",
      "           102,     0,     0,     0,     0,     0],\n",
      "        [  101,  2585,  3976, 19299,  2651,  1012,  1037,  5940,  3442,  2663,\n",
      "          2052,  2275,  1996,  2928,  2005,  1996,  6493,  2663,  9039,  2023,\n",
      "          2161,  1004, 23713,  1025,  2275,  1037,  2630, 18930,  6329,  2501,\n",
      "           102,     0,     0,     0,     0,     0],\n",
      "        [  101,  1045,  2293,  2115,  2609,  1012,  1012,  2009,  1005,  1055,\n",
      "          5875,  1012,  1045,  2001,  2074,  3752,  1037,  2261,  4790,  1012,\n",
      "          1045,  1005,  1049,  1056, 28394,  3436,  1010,  9130,  1010,  8224,\n",
      "          1009,  1996,  3083,  2028,   102,     0],\n",
      "        [  101,  3083,  2051,  2000,  3422, 21563,  1012,  2190,  2835,  2006,\n",
      "          1996,  2160,   999,   999,  2084,  2595,  7842, 12995,   999,  1027,\n",
      "          1007,  1030,  6047, 19027,  7159,  2050, 18338,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2880,  1024,  1030,  5310,  5443,  1012,  1030,  5310,  2112,\n",
      "         24756,  2003,  2183,  2091,  2285, 16318,  2012,  1996, 15418,  2882,\n",
      "          1999,  7136,  1012,  1001, 11966, 16068,  2629,   999,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([0, 2, 1, 0, 0, 2, 1, 0, 1, 2, 1, 0, 2, 2, 2, 1])})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model ready: distilbert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: DataLoader, Model, Optimizer\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "def make_loader(ds, shuffle: bool):\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=cfg.batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=data_collator,\n",
    "    )\n",
    "\n",
    "train_loader = make_loader(train_tok, shuffle=True)\n",
    "val_loader   = make_loader(val_tok, shuffle=False)\n",
    "test_loader  = make_loader(test_tok, shuffle=False)\n",
    "\n",
    "print(\"batch keys:\", next(iter(train_loader)).keys())\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    cfg.model_name,\n",
    "    num_labels=num_labels,\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=cfg.lr)\n",
    "\n",
    "print(\"model ready:\", cfg.model_name)\n",
    "\n",
    "import torch.nn as nn #added\n",
    "criterion = nn.CrossEntropyLoss(weight=weights.to(DEVICE)) #added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b94c28-2970-42f1-8fe7-0149c84a7442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eae286de-2fbe-434a-8802-dbc9f3a625e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: eval_model\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def eval_model(model, dataloader):\n",
    "    \"\"\"\n",
    "    dataloader 全体に対して\n",
    "      - 平均 loss\n",
    "      - accuracy\n",
    "      - macro F1\n",
    "      - all_labels, all_preds\n",
    "    を返す。\n",
    "    \"\"\"\n",
    "\n",
    "    # 0) （デバッグ）dataloader が返す batch のキーを1回だけ確認\n",
    "    try:\n",
    "        first_batch = next(iter(dataloader))\n",
    "        print(\"[eval_model] batch keys:\", first_batch.keys())\n",
    "    except StopIteration:\n",
    "        raise ValueError(\"dataloader is empty (no batches).\")\n",
    "\n",
    "    # 1) model を eval モードにする\n",
    "    model.eval()\n",
    "\n",
    "    # 2) 正解ラベル・予測ラベル用リスト\n",
    "    all_labels = []\n",
    "    all_preds  = []\n",
    "\n",
    "    # 3) loss 用カウンタ\n",
    "    total_loss = 0.0\n",
    "    n_steps = 0\n",
    "\n",
    "    # 4) 勾配は不要\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "\n",
    "            # 4-1) バッチを DEVICE に載せる\n",
    "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "\n",
    "            # 4-2) 順伝播\n",
    "            out = model(**batch)\n",
    "\n",
    "            # 4-3) loss 蓄積\n",
    "            loss = out.loss\n",
    "            total_loss += float(loss.item())\n",
    "            n_steps += 1\n",
    "\n",
    "            # 4-4) 予測ラベル\n",
    "            logits = out.logits\n",
    "            preds = logits.argmax(dim=-1)\n",
    "\n",
    "            # 4-5) labels キー名を吸収する（labels / label 両対応）\n",
    "            if \"labels\" in batch:\n",
    "                labels = batch[\"labels\"]\n",
    "            elif \"label\" in batch:\n",
    "                labels = batch[\"label\"]\n",
    "            else:\n",
    "                raise KeyError(f\"batch has no labels key. keys={list(batch.keys())}\")\n",
    "\n",
    "            # CPU に戻して list に追加\n",
    "            all_labels.extend(labels.detach().cpu().numpy().tolist())\n",
    "            all_preds.extend(preds.detach().cpu().numpy().tolist())\n",
    "\n",
    "    # 5) 平均 loss\n",
    "    avg_loss = total_loss / max(1, n_steps)\n",
    "\n",
    "    # 6) 精度と F1（macro）\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1  = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "\n",
    "    # 7) まとめて返す\n",
    "    return avg_loss, acc, f1, all_labels, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0833b143-6cc8-47a1-bee1-12ad9ddbc7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save dir: /Users/mh/Downloads/Mini Project/Jan_TextClassification_Iteration/models/distilbert_tweet_eval_sentiment_20260109_062409\n",
      "Log   : /Users/mh/Downloads/Mini Project/Jan_TextClassification_Iteration/logs/train_log_20260109_062409.tsv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b737bfee88ef49658fde38c718e29f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/3:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[eval_model] batch keys: KeysView({'input_ids': tensor([[  101,  2601,  9293,  1017,  2258,  4888,  3058,  4484,  2007,  2047,\n",
      "          9117,  1024,  9979,  1996,  4768,  1012,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1000,  2120,  2980,  3899,  2154,  1010,  2120, 26791,  2154,\n",
      "          1010,  2059,  2120,  3153,  2154,  1012,  1012,  1012,  4165,  2066,\n",
      "          1037,  5958,  2305,  1012,  1000,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2043,  3057,  2468,  2316,  4213,  7446,  4599,  1997,  1996,\n",
      "         15285,  2138,  1997,  4302,  1012,  2079,  1061,  1005,  2035,  2130,\n",
      "          2113,  2040,  7158, 15652,  2003,  1029,  2030,  2054,  1037,  3083,\n",
      "          2091,  2003,  1029,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1030,  5310,  1045,  2089,  2030,  2089,  2025,  2031,  9022,\n",
      "          2009,  2039,  2006,  8224,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2182,  1005,  1055,  2115,  3225,  9857,  2851,  2240,  2039,\n",
      "          2012,  7132, 13272,  2007,  6874,  1023,  1024,  2382,  2572,  2000,\n",
      "          2184,  1024,  2382,  2572,  1012,  1012,  1012,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1030,  5310,  1042,  1011,  2364,  1010,  2024,  2017,  1999,\n",
      "          1996,  2436,  4826,  2065,  1045,  4604,  2058,  2070,  9195,  6947,\n",
      "          2015,  1039,  1013,  1051,  2017,  1010,  2005,  2017,  1998,  1037,\n",
      "          2261,  8628,  1029,  1000,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1001,  2149,  3083,  3203,  9393,  8112,  4092,  2012,  1996,\n",
      "          2325,  6012,  1996, 10238,  6465,  2000,  2058,  7558,  2267,  1011,\n",
      "          5391,  2493,  2012,  1996, 20864,  2436,  1012,  1004, 14181,  1025,\n",
      "          1004, 14181,  1025,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 18168,  2290,  2023,  2265,  2003,  2061, 21425,  2130,  2005,\n",
      "          1996,  3822,  4958,  1012, 21766,  2072,  4372,  1032, 23343, 24096,\n",
      "          2683,  2015,  4654,  6898,  2001, 10366,  2005,  4028,  2763,  1032,\n",
      "          1057,  8889,  2475,  2278,  2011,  1996,  4138,  3124,  1012,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1000,  2054,  1037,  2461,  2011,  2703, 26553,  1010,  2204,\n",
      "          6735,  4826,  1998,  1045,  3246,  2017,  2663,  1996,  2330,  1012,\n",
      "          1000,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 12415,  8232, 16392, 26717,  5095,  2305,  1012,  2288,  3132,\n",
      "          7516,  2187,  2006,  1996,  4113,  2862,  1012,  1056, 28394,  2102,\n",
      "          2033,  2339,  2017,  2228,  2017, 10107,  2068,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1030,  5310,  2061,  1996,  2518,  2279,  9432,  3475,  1005,\n",
      "          1056,  2489,  1010,  2017,  1005,  1040,  2031,  2000,  3477,  1002,\n",
      "          2321,  2000,  2131,  1999,  2144,  2017,  2123,  1005,  1056,  2175,\n",
      "          2000,  8529,  9818,  1024,  1013,  1998,  2009,  4515,  2012,  2340,\n",
      "          1024,  2382,  1000,   102,     0,     0,     0,     0,     0],\n",
      "        [  101,  2064,  1005,  1056,  3524,  2005,  4826,  2012,  1023,  7610,\n",
      "           999,  9295,  1058,  6121,  4186,   999,  3246,  1996,  5132,  2097,\n",
      "          2663,   999,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2057,  2074,  2363,  2062,  9735,  2005,  2630, 18936,  2012,\n",
      "          1996, 17710,  2063,  2000, 21451,  5095,  2089,  3708,  1998,  4465,\n",
      "          2089,  3983,  1012,  9735,  1012,  1012,  1012,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 19387,  1030,  5310, 10643,  2245,  2027,  2020,  6047,  2000,\n",
      "          2693,  8112,  1005,  1055,  4613,  2021,  2002,  1005,  2222,  2085,\n",
      "          2031,  1037,  7046,  4378, 16215,  9236,  1012,  2138,  2002,  1005,\n",
      "          2222,  1012,  1012,  1012,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1042,  1008,  1008,  1008,  1996,  7064,  2283,  2023, 10722,\n",
      "          2229, 11053, 19362,  3723, 13821,  1059,  1013,  1030,  5310,  1030,\n",
      "          5310,  1030,  5310,  1030, 11053,  2283,  2160,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1000,  4067,  2017,  1030,  5310,  2005,  1996,  4471,  1012,\n",
      "          1045,  1005,  1049,  2200,  7098,  2000,  2022,  1037, 11290, 14289,\n",
      "         19422,  2937,  1010,  2089,  1045,  2131,  2115,  1042, 14511,  5963,\n",
      "          1029,  1001, 11290, 14289, 19422,  2937,  4135, 21095, 24317,  5794,\n",
      "          4502, 14479,  3022,  1001,  1061,  2078,  4213,  1000,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]]), 'labels': tensor([1, 2, 0, 1, 1, 1, 2, 0, 2, 1, 0, 2, 1, 1, 0, 2])})\n",
      "\n",
      "Epoch 1/3 steps=300 train_loss=0.8587 val_loss=0.7293 val_acc=0.6620 val_f1=0.6440\n",
      "  -> best model updated & saved.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6cc2ff82ae74046ae240b5d06dc2b47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/3:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[eval_model] batch keys: KeysView({'input_ids': tensor([[  101,  2601,  9293,  1017,  2258,  4888,  3058,  4484,  2007,  2047,\n",
      "          9117,  1024,  9979,  1996,  4768,  1012,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1000,  2120,  2980,  3899,  2154,  1010,  2120, 26791,  2154,\n",
      "          1010,  2059,  2120,  3153,  2154,  1012,  1012,  1012,  4165,  2066,\n",
      "          1037,  5958,  2305,  1012,  1000,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2043,  3057,  2468,  2316,  4213,  7446,  4599,  1997,  1996,\n",
      "         15285,  2138,  1997,  4302,  1012,  2079,  1061,  1005,  2035,  2130,\n",
      "          2113,  2040,  7158, 15652,  2003,  1029,  2030,  2054,  1037,  3083,\n",
      "          2091,  2003,  1029,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1030,  5310,  1045,  2089,  2030,  2089,  2025,  2031,  9022,\n",
      "          2009,  2039,  2006,  8224,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2182,  1005,  1055,  2115,  3225,  9857,  2851,  2240,  2039,\n",
      "          2012,  7132, 13272,  2007,  6874,  1023,  1024,  2382,  2572,  2000,\n",
      "          2184,  1024,  2382,  2572,  1012,  1012,  1012,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1030,  5310,  1042,  1011,  2364,  1010,  2024,  2017,  1999,\n",
      "          1996,  2436,  4826,  2065,  1045,  4604,  2058,  2070,  9195,  6947,\n",
      "          2015,  1039,  1013,  1051,  2017,  1010,  2005,  2017,  1998,  1037,\n",
      "          2261,  8628,  1029,  1000,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1001,  2149,  3083,  3203,  9393,  8112,  4092,  2012,  1996,\n",
      "          2325,  6012,  1996, 10238,  6465,  2000,  2058,  7558,  2267,  1011,\n",
      "          5391,  2493,  2012,  1996, 20864,  2436,  1012,  1004, 14181,  1025,\n",
      "          1004, 14181,  1025,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 18168,  2290,  2023,  2265,  2003,  2061, 21425,  2130,  2005,\n",
      "          1996,  3822,  4958,  1012, 21766,  2072,  4372,  1032, 23343, 24096,\n",
      "          2683,  2015,  4654,  6898,  2001, 10366,  2005,  4028,  2763,  1032,\n",
      "          1057,  8889,  2475,  2278,  2011,  1996,  4138,  3124,  1012,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1000,  2054,  1037,  2461,  2011,  2703, 26553,  1010,  2204,\n",
      "          6735,  4826,  1998,  1045,  3246,  2017,  2663,  1996,  2330,  1012,\n",
      "          1000,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 12415,  8232, 16392, 26717,  5095,  2305,  1012,  2288,  3132,\n",
      "          7516,  2187,  2006,  1996,  4113,  2862,  1012,  1056, 28394,  2102,\n",
      "          2033,  2339,  2017,  2228,  2017, 10107,  2068,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1030,  5310,  2061,  1996,  2518,  2279,  9432,  3475,  1005,\n",
      "          1056,  2489,  1010,  2017,  1005,  1040,  2031,  2000,  3477,  1002,\n",
      "          2321,  2000,  2131,  1999,  2144,  2017,  2123,  1005,  1056,  2175,\n",
      "          2000,  8529,  9818,  1024,  1013,  1998,  2009,  4515,  2012,  2340,\n",
      "          1024,  2382,  1000,   102,     0,     0,     0,     0,     0],\n",
      "        [  101,  2064,  1005,  1056,  3524,  2005,  4826,  2012,  1023,  7610,\n",
      "           999,  9295,  1058,  6121,  4186,   999,  3246,  1996,  5132,  2097,\n",
      "          2663,   999,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2057,  2074,  2363,  2062,  9735,  2005,  2630, 18936,  2012,\n",
      "          1996, 17710,  2063,  2000, 21451,  5095,  2089,  3708,  1998,  4465,\n",
      "          2089,  3983,  1012,  9735,  1012,  1012,  1012,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 19387,  1030,  5310, 10643,  2245,  2027,  2020,  6047,  2000,\n",
      "          2693,  8112,  1005,  1055,  4613,  2021,  2002,  1005,  2222,  2085,\n",
      "          2031,  1037,  7046,  4378, 16215,  9236,  1012,  2138,  2002,  1005,\n",
      "          2222,  1012,  1012,  1012,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1042,  1008,  1008,  1008,  1996,  7064,  2283,  2023, 10722,\n",
      "          2229, 11053, 19362,  3723, 13821,  1059,  1013,  1030,  5310,  1030,\n",
      "          5310,  1030,  5310,  1030, 11053,  2283,  2160,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1000,  4067,  2017,  1030,  5310,  2005,  1996,  4471,  1012,\n",
      "          1045,  1005,  1049,  2200,  7098,  2000,  2022,  1037, 11290, 14289,\n",
      "         19422,  2937,  1010,  2089,  1045,  2131,  2115,  1042, 14511,  5963,\n",
      "          1029,  1001, 11290, 14289, 19422,  2937,  4135, 21095, 24317,  5794,\n",
      "          4502, 14479,  3022,  1001,  1061,  2078,  4213,  1000,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]]), 'labels': tensor([1, 2, 0, 1, 1, 1, 2, 0, 2, 1, 0, 2, 1, 1, 0, 2])})\n",
      "\n",
      "Epoch 2/3 steps=300 train_loss=0.6802 val_loss=0.6718 val_acc=0.7040 val_f1=0.6725\n",
      "  -> best model updated & saved.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c926147f0b84ed1b0a732a9ad3cb717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/3:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[eval_model] batch keys: KeysView({'input_ids': tensor([[  101,  2601,  9293,  1017,  2258,  4888,  3058,  4484,  2007,  2047,\n",
      "          9117,  1024,  9979,  1996,  4768,  1012,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1000,  2120,  2980,  3899,  2154,  1010,  2120, 26791,  2154,\n",
      "          1010,  2059,  2120,  3153,  2154,  1012,  1012,  1012,  4165,  2066,\n",
      "          1037,  5958,  2305,  1012,  1000,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2043,  3057,  2468,  2316,  4213,  7446,  4599,  1997,  1996,\n",
      "         15285,  2138,  1997,  4302,  1012,  2079,  1061,  1005,  2035,  2130,\n",
      "          2113,  2040,  7158, 15652,  2003,  1029,  2030,  2054,  1037,  3083,\n",
      "          2091,  2003,  1029,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1030,  5310,  1045,  2089,  2030,  2089,  2025,  2031,  9022,\n",
      "          2009,  2039,  2006,  8224,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2182,  1005,  1055,  2115,  3225,  9857,  2851,  2240,  2039,\n",
      "          2012,  7132, 13272,  2007,  6874,  1023,  1024,  2382,  2572,  2000,\n",
      "          2184,  1024,  2382,  2572,  1012,  1012,  1012,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1030,  5310,  1042,  1011,  2364,  1010,  2024,  2017,  1999,\n",
      "          1996,  2436,  4826,  2065,  1045,  4604,  2058,  2070,  9195,  6947,\n",
      "          2015,  1039,  1013,  1051,  2017,  1010,  2005,  2017,  1998,  1037,\n",
      "          2261,  8628,  1029,  1000,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1001,  2149,  3083,  3203,  9393,  8112,  4092,  2012,  1996,\n",
      "          2325,  6012,  1996, 10238,  6465,  2000,  2058,  7558,  2267,  1011,\n",
      "          5391,  2493,  2012,  1996, 20864,  2436,  1012,  1004, 14181,  1025,\n",
      "          1004, 14181,  1025,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 18168,  2290,  2023,  2265,  2003,  2061, 21425,  2130,  2005,\n",
      "          1996,  3822,  4958,  1012, 21766,  2072,  4372,  1032, 23343, 24096,\n",
      "          2683,  2015,  4654,  6898,  2001, 10366,  2005,  4028,  2763,  1032,\n",
      "          1057,  8889,  2475,  2278,  2011,  1996,  4138,  3124,  1012,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1000,  2054,  1037,  2461,  2011,  2703, 26553,  1010,  2204,\n",
      "          6735,  4826,  1998,  1045,  3246,  2017,  2663,  1996,  2330,  1012,\n",
      "          1000,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 12415,  8232, 16392, 26717,  5095,  2305,  1012,  2288,  3132,\n",
      "          7516,  2187,  2006,  1996,  4113,  2862,  1012,  1056, 28394,  2102,\n",
      "          2033,  2339,  2017,  2228,  2017, 10107,  2068,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1030,  5310,  2061,  1996,  2518,  2279,  9432,  3475,  1005,\n",
      "          1056,  2489,  1010,  2017,  1005,  1040,  2031,  2000,  3477,  1002,\n",
      "          2321,  2000,  2131,  1999,  2144,  2017,  2123,  1005,  1056,  2175,\n",
      "          2000,  8529,  9818,  1024,  1013,  1998,  2009,  4515,  2012,  2340,\n",
      "          1024,  2382,  1000,   102,     0,     0,     0,     0,     0],\n",
      "        [  101,  2064,  1005,  1056,  3524,  2005,  4826,  2012,  1023,  7610,\n",
      "           999,  9295,  1058,  6121,  4186,   999,  3246,  1996,  5132,  2097,\n",
      "          2663,   999,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2057,  2074,  2363,  2062,  9735,  2005,  2630, 18936,  2012,\n",
      "          1996, 17710,  2063,  2000, 21451,  5095,  2089,  3708,  1998,  4465,\n",
      "          2089,  3983,  1012,  9735,  1012,  1012,  1012,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 19387,  1030,  5310, 10643,  2245,  2027,  2020,  6047,  2000,\n",
      "          2693,  8112,  1005,  1055,  4613,  2021,  2002,  1005,  2222,  2085,\n",
      "          2031,  1037,  7046,  4378, 16215,  9236,  1012,  2138,  2002,  1005,\n",
      "          2222,  1012,  1012,  1012,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1042,  1008,  1008,  1008,  1996,  7064,  2283,  2023, 10722,\n",
      "          2229, 11053, 19362,  3723, 13821,  1059,  1013,  1030,  5310,  1030,\n",
      "          5310,  1030,  5310,  1030, 11053,  2283,  2160,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1000,  4067,  2017,  1030,  5310,  2005,  1996,  4471,  1012,\n",
      "          1045,  1005,  1049,  2200,  7098,  2000,  2022,  1037, 11290, 14289,\n",
      "         19422,  2937,  1010,  2089,  1045,  2131,  2115,  1042, 14511,  5963,\n",
      "          1029,  1001, 11290, 14289, 19422,  2937,  4135, 21095, 24317,  5794,\n",
      "          4502, 14479,  3022,  1001,  1061,  2078,  4213,  1000,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]]), 'labels': tensor([1, 2, 0, 1, 1, 1, 2, 0, 2, 1, 0, 2, 1, 1, 0, 2])})\n",
      "\n",
      "Epoch 3/3 steps=300 train_loss=0.5875 val_loss=0.6678 val_acc=0.7040 val_f1=0.6921\n",
      "  -> best model updated & saved.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Training loop (logging + early stopping)\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "ts = nowstamp()\n",
    "run_name = f\"{cfg.project_name}_{ts}\"\n",
    "save_dir = os.path.join(cfg.models_dir, run_name)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "log_path = os.path.join(cfg.logs_dir, f\"train_log_{ts}.tsv\")\n",
    "print(\"Save dir:\", save_dir)\n",
    "print(\"Log   :\", log_path)\n",
    "\n",
    "# 1) ログヘッダ\n",
    "with open(log_path, \"w\") as f:\n",
    "    f.write(\"epoch\\tsteps\\ttrain_loss\\tval_loss\\tval_acc\\tval_f1\\n\")\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for ep in range(1, cfg.epochs + 1):\n",
    "    # 2) train モード\n",
    "    model.train()\n",
    "\n",
    "    # 3) loss集計用\n",
    "    total_loss = 0.0\n",
    "    n_steps = 0\n",
    "\n",
    "    # 4) tqdm\n",
    "    progress = tqdm(train_loader, total=len(train_loader), desc=f\"Epoch {ep}/{cfg.epochs}\")\n",
    "\n",
    "    for step, batch in enumerate(progress):\n",
    "        # 4-1) DEVICEへ\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "\n",
    "        # 4-2) optimizer.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4-3) forward\n",
    "        out = model(**batch)\n",
    "        logits = out.logits  # [B, num_labels]\n",
    "\n",
    "        # 4-4) labels を吸収（labels / label どちらでもOK）\n",
    "        labels = batch[\"labels\"] if \"labels\" in batch else batch[\"label\"]\n",
    "\n",
    "        # 4-5) ★重み付きloss\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        # 4-6) backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 4-7) clip + step\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # 4-8) 集計\n",
    "        total_loss += float(loss.item())\n",
    "        n_steps += 1\n",
    "\n",
    "        # 4-9) tqdm表示\n",
    "        progress.set_postfix({\"loss\": float(loss.item())})\n",
    "\n",
    "        # 4-10) debug step limit\n",
    "        if cfg.debug_max_steps_per_epoch is not None and (step + 1) >= cfg.debug_max_steps_per_epoch:\n",
    "            break\n",
    "\n",
    "    # 5) train_loss（←ここは “for step” の外）\n",
    "    train_loss = total_loss / max(1, n_steps)\n",
    "\n",
    "    # 6) val eval\n",
    "    val_loss, val_acc, val_f1, _, _ = eval_model(model, val_loader)\n",
    "\n",
    "    # 7) MPS cache clear（epoch単位）\n",
    "    if DEVICE == \"mps\":\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "    # 8) print\n",
    "    print(\n",
    "        f\"\\nEpoch {ep}/{cfg.epochs} steps={n_steps} \"\n",
    "        f\"train_loss={train_loss:.4f} val_loss={val_loss:.4f} \"\n",
    "        f\"val_acc={val_acc:.4f} val_f1={val_f1:.4f}\"\n",
    "    )\n",
    "\n",
    "    # 9) log append\n",
    "    with open(log_path, \"a\") as f:\n",
    "        f.write(f\"{ep}\\t{n_steps}\\t{train_loss:.6f}\\t{val_loss:.6f}\\t{val_acc:.6f}\\t{val_f1:.6f}\\n\")\n",
    "\n",
    "    # 10) early stopping + save best\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        model.save_pretrained(save_dir)\n",
    "        tokenizer.save_pretrained(save_dir)\n",
    "        print(\"  -> best model updated & saved.\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"  -> no improvement ({epochs_no_improve}/{cfg.early_stopping_patience})\")\n",
    "        if epochs_no_improve >= cfg.early_stopping_patience:\n",
    "            print(\"  -> Early stopping triggered.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62317800-7174-4c4a-b456-f4cd33d3a771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[eval_model] batch keys: KeysView({'input_ids': tensor([[  101,  1030,  5310,  1030,  5310,  2054,  2079,  2122,  1005,  1015,\n",
      "          1013,  1016,  6248, 27263,  2015,  1005,  2031,  2000,  2079,  2007,\n",
      "          2505,  1029,  2027,  1005,  2128,  2025,  2130,  2066,  2008,  1012,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  2821,  1024,  1523,  1045,  2018,  1037,  2630, 19085,  2096,\n",
      "          1045,  2001,  2023,  1524,  1031,  2652,  2007,  8224,  3011, 27830,\n",
      "          1033,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  1030,  5310,  1030,  5310,  2008,  1005,  1055,  2746,  1010,\n",
      "          2021,  1045,  2228,  1996,  5694,  2024,  2183,  2000,  2022, 19960,\n",
      "          5555,  3593, 15991,  1012,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  1045,  2228,  1045,  2089,  2022,  2633,  1999,  2007,  1996,\n",
      "          1999,  4306,  1001, 10856,  2063, 12519, 18598,  7770,  3351,  1001,\n",
      "         24665, 19303, 11387, 16932,  1030,  5310,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  1030,  5310, 10166,  1010,  2034,  9395, 16860,  1998,  2085,\n",
      "         26000,  2140, 11794,  1012,  6266, 20012,  1010,  2745,  5405,  1010,\n",
      "          6291,  2962,  1010,  1998,  5977,  9502,  2024,  2770,  2041,  1997,\n",
      "          7348,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  7842, 25465, 19767,  2085,  7842, 11905,  4095, 21661,  2165,\n",
      "          4319,  3231,  2444,  2006,  5924,  2694,  1012,  2000,  6011,  2027,\n",
      "          2024,  2025,  4319,  1011, 17999,  8840, 10698,  2229,  1029,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  2129,  2116,  2062,  2420,  2127,  3098,  2154,  1029,   100,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101, 10474,  1005,  1055,  1001,  4067, 29337, 16429,  8067,  3065,\n",
      "          2540, 26675, 15531,  2000,  8962,  2271,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  2035, 20116,  2290,  1998, 25312, 23177,  2035,  2105,  2660,\n",
      "          2003,  2000, 13236,  1998,  5471, 11422,  2218, 26771,   999,  1011,\n",
      "          3696,  1996,  9964,   999,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  1030,  5310,  1030,  5310,  1030,  5310,  1030,  5310,  1030,\n",
      "          5310,  1030,  5310,  2202,  2185,  6206,  2015,  1998,  2757,  2111,\n",
      "          1998,  8398,  5222,  2759,  3789,  2205,  1012,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  2019,  5875,  3036, 18130,  1011, 12167,  2025,  2005,  1996,\n",
      "         10126,  2482, 12383,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  1001, 22999, 15239, 16033,  2361,  2549,  1015,  1012,  3764,\n",
      "          2475,  1012,  3000,  2509,  1012,  6253,  2549,  1012, 17798,  2378,\n",
      "         11365,  7028,  2193,  1019,  1012,  2957,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  1001,  2028,  4305,  2890,  7542,  1001,  4302, 21756,  4244,\n",
      "         10140,  2210,  3153,   100,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  2043,  4575, 26927, 20203, 11254,  7020,  1010, 27615,  1010,\n",
      "         19960,  5555,  3593,  1010,  1004,  2515,  2185,  2007,  9353,  2050,\n",
      "          1010,  2054,  2097,  8398,  1005,  1055,  2918,  2514,  2055,  1000,\n",
      "          2689,  1000,  2059,  1029,  2008,  1005,  1055,  1037,  2502,  2028,\n",
      "          2157,  1029,   999,   102],\n",
      "        [  101, 11963, 16313,  2818, 11808, 21877, 10483,  2072,  7459, 13175,\n",
      "          1005,  2543,  1005,  1999,  1996, 10789, 11963,  1012,  1001,  2304,\n",
      "         27439,  4710,  1030,  5310,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  1030,  5310,  2821, 23644,  7929,  1045,  2156,   100,  2054,\n",
      "          2065,  1057,  2031,  2966, 16204, 14860,  1029,  2515,  2008,  2191,\n",
      "          1037,  4489,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([1, 1, 1, 2, 0, 1, 1, 2, 1, 0, 1, 1, 2, 0, 0, 1])})\n",
      "\n",
      "[Test]\n",
      "loss=0.7292 acc=0.6645 f1_macro=0.6629\n",
      "\n",
      "[Classification Report]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.62      0.71      0.66       616\n",
      "     neutral       0.72      0.63      0.67       995\n",
      "    positive       0.63      0.69      0.66       389\n",
      "\n",
      "    accuracy                           0.66      2000\n",
      "   macro avg       0.66      0.67      0.66      2000\n",
      "weighted avg       0.67      0.66      0.66      2000\n",
      "\n",
      "errors.tsv saved: /Users/mh/Downloads/Mini Project/Jan_TextClassification_Iteration/results/errors_20260109_062409.tsv\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix saved: /Users/mh/Downloads/Mini Project/Jan_TextClassification_Iteration/results/confusion_matrix_20260109_062409.png\n",
      "\n",
      "[Artifacts]\n",
      "best model dir: /Users/mh/Downloads/Mini Project/Jan_TextClassification_Iteration/models/distilbert_tweet_eval_sentiment_20260109_062409\n",
      "train log     : /Users/mh/Downloads/Mini Project/Jan_TextClassification_Iteration/logs/train_log_20260109_062409.tsv\n",
      "errors.tsv    : /Users/mh/Downloads/Mini Project/Jan_TextClassification_Iteration/results/errors_20260109_062409.tsv\n",
      "cm.png        : /Users/mh/Downloads/Mini Project/Jan_TextClassification_Iteration/results/confusion_matrix_20260109_062409.png\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Test eval + errors.tsv + Confusion Matrix\n",
    "\n",
    "# 1) Test評価\n",
    "test_loss, test_acc, test_f1, y_true, y_pred = eval_model(model, test_loader)\n",
    "print(\"\\n[Test]\")\n",
    "print(f\"loss={test_loss:.4f} acc={test_acc:.4f} f1_macro={test_f1:.4f}\")\n",
    "\n",
    "# 2) Classification report（ラベル名があるなら表示）\n",
    "try:\n",
    "    print(\"\\n[Classification Report]\")\n",
    "    print(classification_report(y_true, y_pred, target_names=label_names))\n",
    "except Exception:\n",
    "    print(\"\\n[Classification Report]\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "# 3) errors.tsv（誤分類サンプルを保存）\n",
    "errors_path = os.path.join(cfg.results_dir, f\"errors_{ts}.tsv\")\n",
    "\n",
    "# test_ds は token化前の生データがあるので、それと整合するように注意：\n",
    "# take_n & map の順が同じなので、indexは一致する想定\n",
    "with open(errors_path, \"w\") as f:\n",
    "    f.write(\"idx\\ttrue_id\\tpred_id\\ttrue_label\\tpred_label\\ttext\\n\")\n",
    "    for i, (yt, yp) in enumerate(zip(y_true, y_pred)):\n",
    "        if yt != yp:\n",
    "            ex = test_ds[i]\n",
    "            true_name = label_names[yt] if yt < len(label_names) else str(yt)\n",
    "            pred_name = label_names[yp] if yp < len(label_names) else str(yp)\n",
    "            text = ex[cfg.text_col].replace(\"\\t\", \" \").replace(\"\\n\", \" \")\n",
    "            f.write(f\"{i}\\t{yt}\\t{yp}\\t{true_name}\\t{pred_name}\\t{text}\\n\")\n",
    "\n",
    "print(\"errors.tsv saved:\", errors_path)\n",
    "\n",
    "# 4) Confusion Matrix plot 保存\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "plt.imshow(cm)\n",
    "plt.title(\"Confusion Matrix (tweet_eval sentiment)\")\n",
    "plt.xlabel(\"Pred\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.xticks(range(len(label_names)), label_names, rotation=45, ha=\"right\")\n",
    "plt.yticks(range(len(label_names)), label_names)\n",
    "plt.colorbar()\n",
    "\n",
    "# 値も表示\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\")\n",
    "\n",
    "plt.tight_layout()\n",
    "cm_path = os.path.join(cfg.results_dir, f\"confusion_matrix_{ts}.png\")\n",
    "plt.savefig(cm_path, dpi=200)\n",
    "plt.show()\n",
    "\n",
    "print(\"confusion matrix saved:\", cm_path)\n",
    "\n",
    "# 5) どこに何が出たか、最終まとめ\n",
    "print(\"\\n[Artifacts]\")\n",
    "print(\"best model dir:\", save_dir)\n",
    "print(\"train log     :\", log_path)\n",
    "print(\"errors.tsv    :\", errors_path)\n",
    "print(\"cm.png        :\", cm_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1339fd65-beb4-4250-ad2b-d6c48eeb2364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>pred_label</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true_label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>20</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "pred_label  negative  neutral  positive\n",
       "true_label                             \n",
       "negative           0      144        36\n",
       "neutral          250        0       119\n",
       "positive          20      102         0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "errors = pd.read_csv(\"results/errors_20260109_062409.tsv\", sep=\"\\t\")\n",
    "\n",
    "# 真のラベル × 予測ラベル\n",
    "ct = pd.crosstab(errors[\"true_label\"], errors[\"pred_label\"])\n",
    "ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86d6e3fe-7bd6-48a0-a688-bfe22635f59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)     # 切り捨てない\n",
    "pd.set_option(\"display.max_rows\", 50)           # 必要なら増やす\n",
    "pd.set_option(\"display.max_columns\", 20)\n",
    "pd.set_option(\"display.width\", 200)             # 表示幅（効かなければ次へ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a878f9a0-f682-43d7-b3ce-8a7bcbb0bf5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>true_label</th>\n",
       "      <th>pred_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>Remember The Pretty White Girls Who Became #ISIS Terrorists?You May Wanna Take A Look At Them Now....#OpISIS...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>termites release far more CO2 annually (and by several orders of magnitude) than all the burning of fossil fuels.https://t.co/3BJAsYk8JC</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>We have lost everything: Syrians return to ravaged Aleppo, @user reports. by #AP via @user</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>Fidel Castro's last wish was no monuments in his name.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>@user no I believe u, not denying there was testing on woc. I'm saying animal rts ppl wouldnt support it, which the tweet implies</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>@user tony romo couldn't, you're right, but that's just bc they'd rely too heavily on the QB in stead of the run game.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Is #Ukraine headed for another revolution? #Saakashvili who resigned the Kiev govt because of corruption, thinks yes</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Ahmed lost his home to IS. #MosulOffensive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>@user Imagining a credible threat, can they force him to sit down for the briefing? \"Look son, some bad hombres wanna get you...\"0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>8 Dangerous Side Effects of Fracking That the Industry Doesn't Want You to Hear About | Alternethttps://t.co/NknAwg0MV4</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                         text true_label pred_label\n",
       "430                           Remember The Pretty White Girls Who Became #ISIS Terrorists?You May Wanna Take A Look At Them Now....#OpISIS...    neutral   negative\n",
       "599  termites release far more CO2 annually (and by several orders of magnitude) than all the burning of fossil fuels.https://t.co/3BJAsYk8JC    neutral   negative\n",
       "166                                                We have lost everything: Syrians return to ravaged Aleppo, @user reports. by #AP via @user    neutral   negative\n",
       "663                                                                                    Fidel Castro's last wish was no monuments in his name.    neutral   negative\n",
       "478         @user no I believe u, not denying there was testing on woc. I'm saying animal rts ppl wouldnt support it, which the tweet implies    neutral   negative\n",
       "364                    @user tony romo couldn't, you're right, but that's just bc they'd rely too heavily on the QB in stead of the run game.    neutral   negative\n",
       "150                      Is #Ukraine headed for another revolution? #Saakashvili who resigned the Kiev govt because of corruption, thinks yes    neutral   negative\n",
       "129                                                                                                Ahmed lost his home to IS. #MosulOffensive    neutral   negative\n",
       "93         @user Imagining a credible threat, can they force him to sit down for the briefing? \"Look son, some bad hombres wanna get you...\"0    neutral   negative\n",
       "133                   8 Dangerous Side Effects of Fracking That the Industry Doesn't Want You to Hear About | Alternethttps://t.co/NknAwg0MV4    neutral   negative"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors[\n",
    "    (errors[\"true_label\"] == \"neutral\") &\n",
    "    (errors[\"pred_label\"] == \"negative\")\n",
    "].sample(10)[[\"text\", \"true_label\", \"pred_label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdd6529-9f38-4a2d-8c9b-6ba967450e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0509c4-7d63-4553-9aaf-a365b018ffd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8046a89-4164-4dea-8a90-f3da87dd41ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72ae1f35-14bd-4896-8409-dba6a6966b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>true_label</th>\n",
       "      <th>pred_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>Omfg in china they call kim jong un \"kim fatty the third\"</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>Of course #clownshoes #altright #IllinoisNazis</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>@user @user &amp; to this day every convo w/ Berner you have to redefine 'voter suppression' before we can continue.</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>President-elect Trump could make big bucks from Dakota Access Pipeline, @user reports</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>Sharon needles is doing Melania trump n the crowd is chanting CULEERAA</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>Charles rails on bible thumping conservatives all the time and I find it funny because they're always taken a back that a veteran is going—</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>@user it's free with insurance because of Obamacare, which trump wants to repeal 🙃</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>Steve Bannon look like the type of dude to hang out in front of liquor stores all day handing out \"Hillary is Satan…</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@user @user @user @user @user @user take away illegals and dead people and Trump wins popular vote too.</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>Pop is gay for Tim Duncan</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                            text true_label pred_label\n",
       "264                                                                                    Omfg in china they call kim jong un \"kim fatty the third\"   negative    neutral\n",
       "667                                                                                               Of course #clownshoes #altright #IllinoisNazis   negative    neutral\n",
       "529                             @user @user & to this day every convo w/ Berner you have to redefine 'voter suppression' before we can continue.   negative    neutral\n",
       "294                                                        President-elect Trump could make big bucks from Dakota Access Pipeline, @user reports   negative    neutral\n",
       "176                                                                       Sharon needles is doing Melania trump n the crowd is chanting CULEERAA   negative    neutral\n",
       "365  Charles rails on bible thumping conservatives all the time and I find it funny because they're always taken a back that a veteran is going—   negative    neutral\n",
       "27                                                            @user it's free with insurance because of Obamacare, which trump wants to repeal 🙃   negative    neutral\n",
       "232                         Steve Bannon look like the type of dude to hang out in front of liquor stores all day handing out \"Hillary is Satan…   negative    neutral\n",
       "5                                        @user @user @user @user @user @user take away illegals and dead people and Trump wins popular vote too.   negative    neutral\n",
       "371                                                                                                                    Pop is gay for Tim Duncan   negative    neutral"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors[\n",
    "    (errors[\"true_label\"] == \"negative\") &\n",
    "    (errors[\"pred_label\"] == \"neutral\")\n",
    "].sample(10)[[\"text\", \"true_label\", \"pred_label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e903fd70-3350-4803-93b6-5a89082d5093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 5️⃣ Error analysis (from `errors.tsv`)\n",
      "\n",
      "### Most frequent error patterns\n",
      "- **true=neutral → pred=negative** : **250** samples\n",
      "- **true=negative → pred=neutral** : **144** samples\n",
      "- **true=neutral → pred=positive** : **119** samples\n",
      "- **true=positive → pred=neutral** : **102** samples\n",
      "- **true=negative → pred=positive** : **36** samples\n",
      "\n",
      "### Interpretation (practical)\n",
      "- Errors concentrate around borderline cases (especially involving **neutral**).\n",
      "- Many samples contain *keywords* that look negative/positive, but the overall intent is neutral → the model may over-rely on surface cues.\n",
      "\n",
      "### Next actions\n",
      "- Apply **class weights** (increase weight for `neutral`) and retrain.\n",
      "- Add “human-ambiguous examples” to README to justify remaining errors.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def make_insights_block(errors_df: pd.DataFrame, topn: int = 5) -> str:\n",
    "    # クロスタブ（誤分類だけ）\n",
    "    ct = pd.crosstab(errors_df[\"true_label\"], errors_df[\"pred_label\"])\n",
    "    flat = ct.stack().sort_values(ascending=False)\n",
    "    top = flat.head(topn)\n",
    "\n",
    "    lines = []\n",
    "    lines.append(\"## 5️⃣ Error analysis (from `errors.tsv`)\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"### Most frequent error patterns\")\n",
    "    for (true_lbl, pred_lbl), n in top.items():\n",
    "        lines.append(f\"- **true={true_lbl} → pred={pred_lbl}** : **{int(n)}** samples\")\n",
    "\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"### Interpretation (practical)\")\n",
    "    lines.append(\"- Errors concentrate around borderline cases (especially involving **neutral**).\")\n",
    "    lines.append(\"- Many samples contain *keywords* that look negative/positive, but the overall intent is neutral → the model may over-rely on surface cues.\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"### Next actions\")\n",
    "    lines.append(\"- Apply **class weights** (increase weight for `neutral`) and retrain.\")\n",
    "    lines.append(\"- Add “human-ambiguous examples” to README to justify remaining errors.\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "insights_md = make_insights_block(errors, topn=5)\n",
    "print(insights_md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3260ec8-52e7-4c0c-8560-c3020a477456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: results/insights_from_errors.md\n"
     ]
    }
   ],
   "source": [
    "out_path = \"results/insights_from_errors.md\"\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(insights_md)\n",
    "\n",
    "print(\"saved:\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0405768b-cf70-47fb-bab5-91b3fdb37448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 3636, 2: 3130, 0: 1234})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import torch\n",
    "\n",
    "label_col = cfg.label_col  # たとえば \"label\" のはず\n",
    "counts = Counter(train_ds[label_col])\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5d73585-ca0e-4996-9709-c93ca7d69836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.7305, 0.5873, 0.6822])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ラベルIDは 0..(num_labels-1) を仮定\n",
    "num_labels = len(set(train_ds[label_col]))\n",
    "\n",
    "freq = torch.tensor([counts[i] for i in range(num_labels)], dtype=torch.float)\n",
    "weights = 1.0 / (freq + 1e-9)\n",
    "weights = weights / weights.mean()  # 平均1に正規化（扱いやすい）\n",
    "\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699652a4-b585-440a-a831-2c7ec52bb5b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai-train-2025)",
   "language": "python",
   "name": "ai-train-2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
