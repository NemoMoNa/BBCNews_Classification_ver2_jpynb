{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98f5527b-0941-4bff-99eb-70b83e5de19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: mps\n",
      "CWD: /Users/mh/Downloads/Mini Project/Jan_TextClassification_Iteration\n",
      "dirs: results models logs\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports & Config\n",
    "\n",
    "import os, csv, time, random\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "def nowstamp():\n",
    "    return datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# ---- Config ----\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Project / Paths\n",
    "    project_name: str = \"distilbert_bbcnews_classweight\"\n",
    "    base_dir: str = \"/Users/mh/Downloads/Mini Project/Jan_TextClassification_Iteration\"\n",
    "    results_dir: str = \"results\"\n",
    "    models_dir: str = \"models\"\n",
    "    logs_dir: str = \"logs\"\n",
    "\n",
    "    # Dataset\n",
    "    dataset_name: str = \"SetFit/bbc-news\"   # 安定して使いやすいことが多い\n",
    "    text_col: str = \"text\"\n",
    "    label_col: str = \"label\"               # ←データ次第で \"category\" 等になることあり\n",
    "    val_ratio: float = 0.1\n",
    "\n",
    "    # Model\n",
    "    model_name: str = \"distilbert-base-uncased\"\n",
    "    max_seq_len: int = 128\n",
    "    batch_size: int = 16\n",
    "\n",
    "    # Train\n",
    "    lr: float = 2e-5\n",
    "    epochs: int = 3\n",
    "    seed: int = 42\n",
    "    early_stopping_patience: int = 2\n",
    "    debug_max_steps_per_epoch: int | None = None  # 例: 200 で軽量実行\n",
    "\n",
    "    # Class weight\n",
    "    use_class_weight: bool = False\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "# ---- Device ----\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "# ---- Seed ----\n",
    "random.seed(cfg.seed)\n",
    "np.random.seed(cfg.seed)\n",
    "torch.manual_seed(cfg.seed)\n",
    "\n",
    "# ---- Make dirs ----\n",
    "os.makedirs(cfg.base_dir, exist_ok=True)\n",
    "os.chdir(cfg.base_dir)\n",
    "os.makedirs(cfg.results_dir, exist_ok=True)\n",
    "os.makedirs(cfg.models_dir, exist_ok=True)\n",
    "os.makedirs(cfg.logs_dir, exist_ok=True)\n",
    "\n",
    "print(\"CWD:\", os.getcwd())\n",
    "print(\"dirs:\", cfg.results_dir, cfg.models_dir, cfg.logs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28b894a6-b8aa-497e-b401-ba9504b0adcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 1102 val: 123 test: 1000\n",
      "columns: ['text', 'label', 'label_text']\n",
      "example: {'text': 'wales want rugby league training wales could follow england s lead by training with a rugby league club.  england have already had a three-day session with leeds rhinos  and wales are thought to be interested in a similar clinic with rivals st helens. saints coach ian millward has given his approval  but if it does happen it is unlikely to be this season. saints have a week s training in portugal next week  while wales will play england in the opening six nations match on 5 february.  we have had an approach from wales   confirmed a saints spokesman.  it s in the very early stages but it is something we are giving serious consideration to.  st helens  who are proud of their welsh connections  are obvious partners for the welsh rugby union  despite a spat in 2001 over the collapse of kieron cunningham s proposed £500 000 move to union side swansea. a similar cross-code deal that took iestyn harris from leeds to cardiff in 2001 did go through  before the talented stand-off returned to the 13-man code with bradford bulls. kel coslett  who famously moved from wales to league in the 1960s  is currently saints  football manager  while clive griffiths - wales  defensive coach - is a former st helens player and is thought to be the man behind the latest initiative. scott gibbs  the former wales and lions centre  played for st helens from 1994-96 and was in the challenge cup-winning team at wembley in 1996.', 'label': 2}\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Dataset load & split\n",
    "\n",
    "raw = load_dataset(cfg.dataset_name)\n",
    "\n",
    "# 多くのHFデータは train/test だけ、あるいは train/validation/test がある\n",
    "if \"validation\" in raw:\n",
    "    train_full = raw[\"train\"]\n",
    "    val_ds = raw[\"validation\"]\n",
    "    test_ds = raw[\"test\"] if \"test\" in raw else None\n",
    "else:\n",
    "    # train から val を作る\n",
    "    train_full = raw[\"train\"].shuffle(seed=cfg.seed)\n",
    "    split = train_full.train_test_split(test_size=cfg.val_ratio, seed=cfg.seed)\n",
    "    train_full = split[\"train\"]\n",
    "    val_ds = split[\"test\"]\n",
    "    test_ds = raw[\"test\"] if \"test\" in raw else None\n",
    "\n",
    "print(\"train:\", len(train_full), \"val:\", len(val_ds), \"test:\", (len(test_ds) if test_ds is not None else None))\n",
    "print(\"columns:\", train_full.column_names)\n",
    "\n",
    "# label_col が想定と違う場合に備えて、候補を表示\n",
    "print(\"example:\", {k: train_full[0][k] for k in train_full.column_names if k in [cfg.text_col, cfg.label_col]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1491df32-421c-4415-b3ae-95e95bb712c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label feature: Value('int64')\n",
      "num_labels: 5\n",
      "labels: ['0', '1', '2', '3', '4']\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Ensure label is ClassLabel (encode if needed)\n",
    "\n",
    "from datasets import ClassLabel\n",
    "\n",
    "label_feature = train_full.features.get(cfg.label_col, None)\n",
    "print(\"label feature:\", label_feature)\n",
    "\n",
    "# label が string(Value) の場合は class_encode_column をかける\n",
    "# これで train_full.features[label_col].num_classes が使えるようになる\n",
    "if not isinstance(label_feature, ClassLabel):\n",
    "    train_full = train_full.class_encode_column(cfg.label_col)\n",
    "    val_ds = val_ds.class_encode_column(cfg.label_col)\n",
    "    if test_ds is not None:\n",
    "        test_ds = test_ds.class_encode_column(cfg.label_col)\n",
    "\n",
    "label_feature = train_full.features[cfg.label_col]\n",
    "num_labels = label_feature.num_classes\n",
    "label_names = label_feature.names\n",
    "\n",
    "print(\"num_labels:\", num_labels)\n",
    "print(\"labels:\", label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "123c8491-6f56-436e-b673-9e358909bd4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24bf61f4dba24a03b989123cf6baad5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/123 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "batch shapes: torch.Size([16, 128])\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Tokenize & DataLoader\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_name, use_fast=True)\n",
    "\n",
    "def tokenize_batch(examples):\n",
    "    enc = tokenizer(\n",
    "        examples[cfg.text_col],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=cfg.max_seq_len,\n",
    "    )\n",
    "    # ★重要：学習用キーは \"labels\" に統一する（HuggingFaceが期待）\n",
    "    enc[\"labels\"] = examples[cfg.label_col]\n",
    "    return enc\n",
    "\n",
    "train_tok = train_full.map(tokenize_batch, batched=True, remove_columns=train_full.column_names)\n",
    "val_tok   = val_ds.map(tokenize_batch, batched=True, remove_columns=val_ds.column_names)\n",
    "\n",
    "cols = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "train_tok.set_format(type=\"torch\", columns=cols)\n",
    "val_tok.set_format(type=\"torch\", columns=cols)\n",
    "\n",
    "train_loader = DataLoader(train_tok, batch_size=cfg.batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_tok, batch_size=cfg.batch_size, shuffle=False)\n",
    "\n",
    "print(\"batch keys:\", next(iter(val_loader)).keys())\n",
    "print(\"batch shapes:\", next(iter(val_loader))[\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44d60b91-eb2c-46b1-978d-dbdc8a106f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model ready: distilbert-base-uncased\n",
      "counts: Counter({1: 264, 2: 248, 4: 222, 0: 184, 3: 184})\n",
      "freq: [184.0, 264.0, 248.0, 184.0, 222.0]\n",
      "weights: [1.1715819835662842, 0.8165571093559265, 0.8692381381988525, 1.1715819835662842, 0.9710409045219421]\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Model & Optimizer (+ class weight)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    cfg.model_name,\n",
    "    num_labels=num_labels,\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=cfg.lr)\n",
    "print(\"model ready:\", cfg.model_name)\n",
    "\n",
    "# ---- class weights ----\n",
    "# counts を train_full（ラベル列）から作る\n",
    "counts = Counter(train_full[cfg.label_col])  # 0..num_labels-1 の頻度\n",
    "\n",
    "# ラベルIDは 0..(num_labels-1) を仮定\n",
    "freq = torch.tensor([counts[i] for i in range(num_labels)], dtype=torch.float)\n",
    "\n",
    "weights = 1.0 / (freq + 1e-9)\n",
    "weights = weights / weights.mean()  # 平均1に正規化（扱いやすい）\n",
    "\n",
    "print(\"counts:\", counts)\n",
    "print(\"freq:\", freq.tolist())\n",
    "print(\"weights:\", weights.tolist())\n",
    "\n",
    "# criterion（重み付きCrossEntropyLoss）\n",
    "if cfg.use_class_weight:\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights.to(DEVICE))\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ea1060e-c2f3-4ac9-9cdf-a79d992ab9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: eval_model\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def eval_model(model, dataloader):\n",
    "    \"\"\"\n",
    "    dataloader 全体に対して\n",
    "      - 平均 loss\n",
    "      - accuracy\n",
    "      - macro F1\n",
    "      - all_labels, all_preds\n",
    "    を返す。\n",
    "    \"\"\"\n",
    "    # 1) model を eval モード\n",
    "    model.eval()\n",
    "\n",
    "    # 2) 正解ラベル・予測ラベル用リスト\n",
    "    all_labels, all_preds = [], []\n",
    "\n",
    "    # 3) loss 集計用\n",
    "    total_loss, n_steps = 0.0, 0\n",
    "\n",
    "    # 4) 勾配は不要\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # 4-1) DEVICE へ転送\n",
    "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "\n",
    "            # 4-2) forward（HuggingFaceモデルへ）\n",
    "            out = model(**batch)\n",
    "            logits = out.logits  # [B, num_labels]\n",
    "\n",
    "            # 4-3) loss を取得（原則 out.loss でもOKだが、criterionで統一してもよい）\n",
    "            # ここでは「学習と同じ定義」に揃えるため criterion を使う\n",
    "            # （class weight を eval loss にも反映したい場合）\n",
    "            labels = batch[\"labels\"] if \"labels\" in batch else batch[\"label\"]\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            total_loss += float(loss.item())\n",
    "            n_steps += 1\n",
    "\n",
    "            # 4-4) preds を作る\n",
    "            preds = logits.argmax(dim=-1)\n",
    "\n",
    "            # 4-5) CPUへ戻して list に蓄積\n",
    "            all_labels.extend(labels.detach().cpu().numpy().tolist())\n",
    "            all_preds.extend(preds.detach().cpu().numpy().tolist())\n",
    "\n",
    "    # 5) 平均 loss\n",
    "    avg_loss = total_loss / max(1, n_steps)\n",
    "\n",
    "    # 6) 指標\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1  = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "\n",
    "    # 7) return\n",
    "    return avg_loss, acc, f1, all_labels, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb9d81ba-5099-4fb7-b2fb-12f2c7bfe1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save dir: models/distilbert_bbcnews_classweight_20260112_182403\n",
      "Log   : logs/train_log_20260112_182403.tsv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a49aacf351bd4fca83cef61e3a9bd980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/3:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3 steps=69 train_loss=0.8877 val_loss=0.2056 val_acc=0.9919 val_f1=0.9904\n",
      "  -> best model updated & saved.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a67b6612d90e4ca9a9eaca2528e31e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/3:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/3 steps=69 train_loss=0.1297 val_loss=0.0862 val_acc=0.9837 val_f1=0.9830\n",
      "  -> best model updated & saved.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8773e006efa5445cb4bbce9862c6d36e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/3:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/3 steps=69 train_loss=0.0294 val_loss=0.0574 val_acc=0.9919 val_f1=0.9904\n",
      "  -> best model updated & saved.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Training loop（logging + early stopping）\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "ts = nowstamp()\n",
    "run_name = f\"{cfg.project_name}_{ts}\"\n",
    "save_dir = os.path.join(cfg.models_dir, run_name)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "log_path = os.path.join(cfg.logs_dir, f\"train_log_{ts}.tsv\")\n",
    "print(\"Save dir:\", save_dir)\n",
    "print(\"Log   :\", log_path)\n",
    "\n",
    "with open(log_path, \"w\") as f:\n",
    "    f.write(\"epoch\\tsteps\\ttrain_loss\\tval_loss\\tval_acc\\tval_f1\\n\")\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for ep in range(1, cfg.epochs + 1):\n",
    "    model.train()\n",
    "    total_loss, n_steps = 0.0, 0\n",
    "\n",
    "    progress = tqdm(train_loader, total=len(train_loader), desc=f\"Epoch {ep}/{cfg.epochs}\")\n",
    "\n",
    "    for step, batch in enumerate(progress):\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = model(**batch)\n",
    "        logits = out.logits\n",
    "        labels = batch[\"labels\"] if \"labels\" in batch else batch[\"label\"]\n",
    "\n",
    "        # ★重み付きloss（または通常loss）\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss.item())\n",
    "        n_steps += 1\n",
    "\n",
    "        progress.set_postfix({\"loss\": float(loss.item())})\n",
    "\n",
    "        if cfg.debug_max_steps_per_epoch is not None and (step + 1) >= cfg.debug_max_steps_per_epoch:\n",
    "            break\n",
    "\n",
    "    train_loss = total_loss / max(1, n_steps)\n",
    "    val_loss, val_acc, val_f1, _, _ = eval_model(model, val_loader)\n",
    "\n",
    "    if DEVICE == \"mps\":\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "    print(f\"\\nEpoch {ep}/{cfg.epochs} steps={n_steps} \"\n",
    "          f\"train_loss={train_loss:.4f} val_loss={val_loss:.4f} \"\n",
    "          f\"val_acc={val_acc:.4f} val_f1={val_f1:.4f}\")\n",
    "\n",
    "    with open(log_path, \"a\") as f:\n",
    "        f.write(f\"{ep}\\t{n_steps}\\t{train_loss:.6f}\\t{val_loss:.6f}\\t{val_acc:.6f}\\t{val_f1:.6f}\\n\")\n",
    "\n",
    "    # Early stopping & best model save\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        model.save_pretrained(save_dir)\n",
    "        tokenizer.save_pretrained(save_dir)\n",
    "        print(\"  -> best model updated & saved.\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"  -> no improvement ({epochs_no_improve}/{cfg.early_stopping_patience})\")\n",
    "        if epochs_no_improve >= cfg.early_stopping_patience:\n",
    "            print(\"  -> Early stopping triggered.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7468f775-dc28-4c68-a181-470a86e3fa41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL val_loss: 0.05743349879048765 val_acc: 0.991869918699187 val_f1: 0.9904273504273504\n",
      "Saved: results/errors_20260112_182403.tsv\n",
      "Saved: results/confusion_20260112_182403.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        28\n",
      "           1       0.96      1.00      0.98        22\n",
      "           2       1.00      1.00      1.00        27\n",
      "           3       1.00      1.00      1.00        26\n",
      "           4       1.00      0.95      0.97        20\n",
      "\n",
      "    accuracy                           0.99       123\n",
      "   macro avg       0.99      0.99      0.99       123\n",
      "weighted avg       0.99      0.99      0.99       123\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: errors.tsv + confusion matrix\n",
    "\n",
    "# ベストモデルをロードし直す（保存済みを使って評価を安定させる）\n",
    "best_model = AutoModelForSequenceClassification.from_pretrained(save_dir).to(DEVICE)\n",
    "\n",
    "val_loss, val_acc, val_f1, y_true, y_pred = eval_model(best_model, val_loader)\n",
    "\n",
    "print(\"FINAL val_loss:\", val_loss, \"val_acc:\", val_acc, \"val_f1:\", val_f1)\n",
    "\n",
    "# val 元データ（テキスト/ラベル）も参照できるように index を揃える\n",
    "val_texts = val_ds[cfg.text_col]\n",
    "val_labels = val_ds[cfg.label_col]  # int\n",
    "\n",
    "# errors.tsv\n",
    "errors_path = os.path.join(cfg.results_dir, f\"errors_{ts}.tsv\")\n",
    "with open(errors_path, \"w\", newline=\"\") as f:\n",
    "    w = csv.writer(f, delimiter=\"\\t\")\n",
    "    w.writerow([\"idx\", \"text\", \"true_id\", \"true_label\", \"pred_id\", \"pred_label\"])\n",
    "    for i, (t, p) in enumerate(zip(y_true, y_pred)):\n",
    "        if t != p:\n",
    "            w.writerow([i, val_texts[i], t, label_names[t], p, label_names[p]])\n",
    "\n",
    "print(\"Saved:\", errors_path)\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(range(num_labels)))\n",
    "cm_path = os.path.join(cfg.results_dir, f\"confusion_{ts}.csv\")\n",
    "pd.DataFrame(cm, index=label_names, columns=label_names).to_csv(cm_path)\n",
    "\n",
    "print(\"Saved:\", cm_path)\n",
    "\n",
    "# ついでに簡易レポート\n",
    "print(classification_report(y_true, y_pred, target_names=label_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369b4cc0-3437-402d-adea-287169c81ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai-train-2025)",
   "language": "python",
   "name": "ai-train-2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
